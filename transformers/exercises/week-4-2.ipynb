{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 在“LoRA 低秩适配 OpenAI Whisper-Large-V2 语音识别任务”中，为中文语料的训练过程增加过程评估，观察 Train Loss 和 Validation Loss 变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"../../../datasets/common_voice_11_0\"\n",
    "\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/python-env/learn-llm/lib/python3.10/site-packages/datasets/load.py:922: FutureWarning: The repository for common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at ../../../datasets/common_voice_11_0/common_voice_11_0.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/sheng/python-env/learn-llm/lib/python3.10/site-packages/datasets/load.py:922: FutureWarning: The repository for common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at ../../../datasets/common_voice_11_0/common_voice_11_0.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'client_id': '95368aab163e0387e4fd4991b4f2d8ccfbd4364bf656c860230501fd27dcedf087773e4695a6cf5de9c4f1d406d582283190d065cdfa36b0e2b060cffaca977e',\n",
       " 'path': '/home/sheng/.cache/huggingface/datasets/downloads/extracted/cecfe1d6518799fb4de108d2f7e9d0831b4c648d9298b8a7079afa6627c2779e/zh-CN_train_0/common_voice_zh-CN_33211591.mp3',\n",
       " 'audio': {'path': '/home/sheng/.cache/huggingface/datasets/downloads/extracted/cecfe1d6518799fb4de108d2f7e9d0831b4c648d9298b8a7079afa6627c2779e/zh-CN_train_0/common_voice_zh-CN_33211591.mp3',\n",
       "  'array': array([-1.13686838e-13, -8.81072992e-13, -1.50635060e-12, ...,\n",
       "         -1.20806035e-05, -9.51409311e-06, -4.57577698e-06]),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': '汉代经济先后经历了繁荣和衰落两个极端的阶段。',\n",
       " 'up_votes': 3,\n",
       " 'down_votes': 2,\n",
       " 'age': '',\n",
       " 'gender': '',\n",
       " 'accent': '',\n",
       " 'locale': 'zh-CN',\n",
       " 'segment': ''}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train[:20%]\")\n",
    "common_voice[\"validation\"] = load_dataset(dataset_name, language_abbr, split=\"test[:1%]\")\n",
    "common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test[:20%]\")\n",
    "common_voice[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, language=language, task=task)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/home/sheng/.cache/huggingface/datasets/downloads/extracted/cecfe1d6518799fb4de108d2f7e9d0831b4c648d9298b8a7079afa6627c2779e/zh-CN_train_0/common_voice_zh-CN_33211591.mp3',\n",
       "  'array': array([-1.13686838e-13, -8.81072992e-13, -1.50635060e-12, ...,\n",
       "         -1.20806035e-05, -9.51409311e-06, -4.57577698e-06]),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': '汉代经济先后经历了繁荣和衰落两个极端的阶段。'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")\n",
    "common_voice[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8515ec07256242c29c6d7e4a756a17cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f1140158884b06868288930aa30759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/python-env/learn-llm/lib/python3.10/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_int8_training\n",
    "\n",
    "model = prepare_model_for_int8_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,932,160 || all params: 1,547,237,120 || trainable%: 0.25414074863974306\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# 设置序列到序列模型训练的参数\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"models/whisper-large-v2-asr-int8\",  # 指定模型输出和保存的目录\n",
    "    per_device_train_batch_size=batch_size,  # 每个设备上的训练批量大小\n",
    "    gradient_accumulation_steps=1,  # 梯度累积步数，在每次优化器步骤之前累积的更新步数\n",
    "    learning_rate=1e-4,  # 学习率\n",
    "    warmup_steps=10,  # 在训练初期增加学习率的步数，有助于稳定训练\n",
    "    # max_steps=620, # 训练总步数\n",
    "    evaluation_strategy=\"steps\",  # 设置评估策略，这里是在每个steps结束时进行评估\n",
    "    eval_steps=10,  # 指定评估的步数\n",
    "    num_train_epochs=3,  # 训练的总轮数\n",
    "    # evaluation_strategy=\"epoch\",  # 设置评估策略，这里是在每个epoch结束时进行评估\n",
    "    fp16=True,  # 启用混合精度训练，可以提高训练速度，同时减少内存使用\n",
    "    per_device_eval_batch_size=batch_size,  # 每个设备上的评估批量大小\n",
    "    generation_max_length=64,  # 生成任务的最大长度\n",
    "    logging_steps=3,  # 指定日志记录的步骤，用于跟踪训练进度\n",
    "    remove_unused_columns=False,  # 是否删除不使用的列，以减少数据处理开销\n",
    "    label_names=[\"labels\"],  # 指定标签列的名称，用于训练过程中\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from transformers import Seq2SeqTrainer, TrainerCallback, Seq2SeqTrainingArguments, TrainerState, TrainerControl\n",
    "import os\n",
    "\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: Seq2SeqTrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 5811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 106\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 2116\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/python-env/learn-llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/sheng/python-env/learn-llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/sheng/python-env/learn-llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 1:57:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.256800</td>\n",
       "      <td>2.377071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.150300</td>\n",
       "      <td>1.833557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.798500</td>\n",
       "      <td>1.634162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.528900</td>\n",
       "      <td>1.475671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.436400</td>\n",
       "      <td>1.329411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.166800</td>\n",
       "      <td>1.203673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>1.034515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.790400</td>\n",
       "      <td>0.827447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.548200</td>\n",
       "      <td>0.803091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.631100</td>\n",
       "      <td>0.796305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.618100</td>\n",
       "      <td>0.791212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.576400</td>\n",
       "      <td>0.790036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.649700</td>\n",
       "      <td>0.786581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.602300</td>\n",
       "      <td>0.781957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.666500</td>\n",
       "      <td>0.777371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.623000</td>\n",
       "      <td>0.773202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.693400</td>\n",
       "      <td>0.773758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.773151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.674900</td>\n",
       "      <td>0.774151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.758400</td>\n",
       "      <td>0.775271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.620800</td>\n",
       "      <td>0.773536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.775588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.723300</td>\n",
       "      <td>0.777407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.636700</td>\n",
       "      <td>0.773437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.614800</td>\n",
       "      <td>0.771519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.734800</td>\n",
       "      <td>0.773569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.680500</td>\n",
       "      <td>0.774444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.733500</td>\n",
       "      <td>0.770485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.433200</td>\n",
       "      <td>0.766431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.424900</td>\n",
       "      <td>0.762612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.517900</td>\n",
       "      <td>0.761518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.527700</td>\n",
       "      <td>0.763408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.678800</td>\n",
       "      <td>0.765243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.629600</td>\n",
       "      <td>0.762545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.415300</td>\n",
       "      <td>0.760111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.571400</td>\n",
       "      <td>0.762033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.621100</td>\n",
       "      <td>0.765522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.477700</td>\n",
       "      <td>0.766513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.541100</td>\n",
       "      <td>0.765086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>0.764016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.669600</td>\n",
       "      <td>0.764156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>0.764773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.702100</td>\n",
       "      <td>0.763840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>0.764770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.546600</td>\n",
       "      <td>0.766792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.693400</td>\n",
       "      <td>0.764603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>0.766973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.532400</td>\n",
       "      <td>0.766583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.551300</td>\n",
       "      <td>0.765271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.686200</td>\n",
       "      <td>0.763680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.604300</td>\n",
       "      <td>0.765114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.638500</td>\n",
       "      <td>0.765244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.565200</td>\n",
       "      <td>0.763272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.491900</td>\n",
       "      <td>0.764800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/python-env/learn-llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/sheng/python-env/learn-llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/sheng/python-env/learn-llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=546, training_loss=0.7803414206365089, metrics={'train_runtime': 7106.477, 'train_samples_per_second': 2.453, 'train_steps_per_second': 0.077, 'total_flos': 3.71119514628096e+19, 'train_loss': 0.7803414206365089, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/whisper-large-v2-asr-int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 4.8248, 'learning_rate': 1e-05, 'epoch': 0.02, 'step': 3},\n",
       " {'loss': 4.9368, 'learning_rate': 4e-05, 'epoch': 0.03, 'step': 6},\n",
       " {'loss': 3.2568, 'learning_rate': 7e-05, 'epoch': 0.05, 'step': 9},\n",
       " {'loss': 2.3589, 'learning_rate': 0.0001, 'epoch': 0.07, 'step': 12},\n",
       " {'loss': 2.0912,\n",
       "  'learning_rate': 9.94402985074627e-05,\n",
       "  'epoch': 0.08,\n",
       "  'step': 15},\n",
       " {'loss': 2.1503,\n",
       "  'learning_rate': 9.888059701492539e-05,\n",
       "  'epoch': 0.1,\n",
       "  'step': 18},\n",
       " {'loss': 2.0475,\n",
       "  'learning_rate': 9.832089552238806e-05,\n",
       "  'epoch': 0.12,\n",
       "  'step': 21},\n",
       " {'loss': 1.6087,\n",
       "  'learning_rate': 9.776119402985075e-05,\n",
       "  'epoch': 0.13,\n",
       "  'step': 24},\n",
       " {'loss': 1.9261,\n",
       "  'learning_rate': 9.720149253731343e-05,\n",
       "  'epoch': 0.15,\n",
       "  'step': 27},\n",
       " {'loss': 1.7985,\n",
       "  'learning_rate': 9.664179104477612e-05,\n",
       "  'epoch': 0.16,\n",
       "  'step': 30},\n",
       " {'loss': 1.6785,\n",
       "  'learning_rate': 9.608208955223881e-05,\n",
       "  'epoch': 0.18,\n",
       "  'step': 33},\n",
       " {'loss': 1.5311,\n",
       "  'learning_rate': 9.552238805970149e-05,\n",
       "  'epoch': 0.2,\n",
       "  'step': 36},\n",
       " {'loss': 1.5289,\n",
       "  'learning_rate': 9.496268656716418e-05,\n",
       "  'epoch': 0.21,\n",
       "  'step': 39},\n",
       " {'loss': 1.3975,\n",
       "  'learning_rate': 9.440298507462687e-05,\n",
       "  'epoch': 0.23,\n",
       "  'step': 42},\n",
       " {'loss': 1.3452,\n",
       "  'learning_rate': 9.384328358208956e-05,\n",
       "  'epoch': 0.25,\n",
       "  'step': 45},\n",
       " {'loss': 1.4364,\n",
       "  'learning_rate': 9.328358208955224e-05,\n",
       "  'epoch': 0.26,\n",
       "  'step': 48},\n",
       " {'loss': 1.4511,\n",
       "  'learning_rate': 9.272388059701493e-05,\n",
       "  'epoch': 0.28,\n",
       "  'step': 51},\n",
       " {'loss': 1.4474,\n",
       "  'learning_rate': 9.216417910447762e-05,\n",
       "  'epoch': 0.3,\n",
       "  'step': 54},\n",
       " {'loss': 1.084,\n",
       "  'learning_rate': 9.16044776119403e-05,\n",
       "  'epoch': 0.31,\n",
       "  'step': 57},\n",
       " {'loss': 1.1668,\n",
       "  'learning_rate': 9.104477611940299e-05,\n",
       "  'epoch': 0.33,\n",
       "  'step': 60},\n",
       " {'loss': 1.0293,\n",
       "  'learning_rate': 9.048507462686568e-05,\n",
       "  'epoch': 0.35,\n",
       "  'step': 63},\n",
       " {'loss': 1.1141,\n",
       "  'learning_rate': 8.992537313432836e-05,\n",
       "  'epoch': 0.36,\n",
       "  'step': 66},\n",
       " {'loss': 0.962,\n",
       "  'learning_rate': 8.936567164179105e-05,\n",
       "  'epoch': 0.38,\n",
       "  'step': 69},\n",
       " {'loss': 0.9594,\n",
       "  'learning_rate': 8.880597014925374e-05,\n",
       "  'epoch': 0.4,\n",
       "  'step': 72},\n",
       " {'loss': 0.7714,\n",
       "  'learning_rate': 8.824626865671643e-05,\n",
       "  'epoch': 0.41,\n",
       "  'step': 75},\n",
       " {'loss': 0.7904,\n",
       "  'learning_rate': 8.76865671641791e-05,\n",
       "  'epoch': 0.43,\n",
       "  'step': 78},\n",
       " {'loss': 0.7888,\n",
       "  'learning_rate': 8.71268656716418e-05,\n",
       "  'epoch': 0.45,\n",
       "  'step': 81},\n",
       " {'loss': 0.9568,\n",
       "  'learning_rate': 8.656716417910447e-05,\n",
       "  'epoch': 0.46,\n",
       "  'step': 84},\n",
       " {'loss': 0.7795,\n",
       "  'learning_rate': 8.600746268656717e-05,\n",
       "  'epoch': 0.48,\n",
       "  'step': 87},\n",
       " {'loss': 0.5482,\n",
       "  'learning_rate': 8.544776119402986e-05,\n",
       "  'epoch': 0.49,\n",
       "  'step': 90},\n",
       " {'loss': 0.6086,\n",
       "  'learning_rate': 8.488805970149253e-05,\n",
       "  'epoch': 0.51,\n",
       "  'step': 93},\n",
       " {'loss': 0.8259,\n",
       "  'learning_rate': 8.432835820895522e-05,\n",
       "  'epoch': 0.53,\n",
       "  'step': 96},\n",
       " {'loss': 0.6311,\n",
       "  'learning_rate': 8.376865671641791e-05,\n",
       "  'epoch': 0.54,\n",
       "  'step': 99},\n",
       " {'loss': 0.5348,\n",
       "  'learning_rate': 8.32089552238806e-05,\n",
       "  'epoch': 0.56,\n",
       "  'step': 102},\n",
       " {'loss': 0.6761,\n",
       "  'learning_rate': 8.26492537313433e-05,\n",
       "  'epoch': 0.58,\n",
       "  'step': 105},\n",
       " {'loss': 0.6181,\n",
       "  'learning_rate': 8.208955223880597e-05,\n",
       "  'epoch': 0.59,\n",
       "  'step': 108},\n",
       " {'loss': 0.6697,\n",
       "  'learning_rate': 8.152985074626866e-05,\n",
       "  'epoch': 0.61,\n",
       "  'step': 111},\n",
       " {'loss': 0.5984,\n",
       "  'learning_rate': 8.097014925373134e-05,\n",
       "  'epoch': 0.63,\n",
       "  'step': 114},\n",
       " {'loss': 0.7685,\n",
       "  'learning_rate': 8.041044776119403e-05,\n",
       "  'epoch': 0.64,\n",
       "  'step': 117},\n",
       " {'loss': 0.5764,\n",
       "  'learning_rate': 7.985074626865672e-05,\n",
       "  'epoch': 0.66,\n",
       "  'step': 120},\n",
       " {'loss': 0.5917,\n",
       "  'learning_rate': 7.92910447761194e-05,\n",
       "  'epoch': 0.68,\n",
       "  'step': 123},\n",
       " {'loss': 0.552,\n",
       "  'learning_rate': 7.873134328358209e-05,\n",
       "  'epoch': 0.69,\n",
       "  'step': 126},\n",
       " {'loss': 0.6497,\n",
       "  'learning_rate': 7.817164179104478e-05,\n",
       "  'epoch': 0.71,\n",
       "  'step': 129},\n",
       " {'loss': 0.5268,\n",
       "  'learning_rate': 7.761194029850747e-05,\n",
       "  'epoch': 0.73,\n",
       "  'step': 132},\n",
       " {'loss': 0.6724,\n",
       "  'learning_rate': 7.705223880597015e-05,\n",
       "  'epoch': 0.74,\n",
       "  'step': 135},\n",
       " {'loss': 0.6023,\n",
       "  'learning_rate': 7.649253731343284e-05,\n",
       "  'epoch': 0.76,\n",
       "  'step': 138},\n",
       " {'loss': 0.7716,\n",
       "  'learning_rate': 7.593283582089553e-05,\n",
       "  'epoch': 0.77,\n",
       "  'step': 141},\n",
       " {'loss': 0.4314,\n",
       "  'learning_rate': 7.537313432835821e-05,\n",
       "  'epoch': 0.79,\n",
       "  'step': 144},\n",
       " {'loss': 0.5489,\n",
       "  'learning_rate': 7.48134328358209e-05,\n",
       "  'epoch': 0.81,\n",
       "  'step': 147},\n",
       " {'loss': 0.6665,\n",
       "  'learning_rate': 7.425373134328359e-05,\n",
       "  'epoch': 0.82,\n",
       "  'step': 150},\n",
       " {'loss': 0.7777,\n",
       "  'learning_rate': 7.369402985074627e-05,\n",
       "  'epoch': 0.84,\n",
       "  'step': 153},\n",
       " {'loss': 0.5081,\n",
       "  'learning_rate': 7.313432835820896e-05,\n",
       "  'epoch': 0.86,\n",
       "  'step': 156},\n",
       " {'loss': 0.623,\n",
       "  'learning_rate': 7.257462686567165e-05,\n",
       "  'epoch': 0.87,\n",
       "  'step': 159},\n",
       " {'loss': 0.8353,\n",
       "  'learning_rate': 7.201492537313434e-05,\n",
       "  'epoch': 0.89,\n",
       "  'step': 162},\n",
       " {'loss': 0.5737,\n",
       "  'learning_rate': 7.145522388059702e-05,\n",
       "  'epoch': 0.91,\n",
       "  'step': 165},\n",
       " {'loss': 0.6934,\n",
       "  'learning_rate': 7.089552238805971e-05,\n",
       "  'epoch': 0.92,\n",
       "  'step': 168},\n",
       " {'loss': 0.9197,\n",
       "  'learning_rate': 7.033582089552238e-05,\n",
       "  'epoch': 0.94,\n",
       "  'step': 171},\n",
       " {'loss': 0.6781,\n",
       "  'learning_rate': 6.977611940298508e-05,\n",
       "  'epoch': 0.96,\n",
       "  'step': 174},\n",
       " {'loss': 0.6488,\n",
       "  'learning_rate': 6.921641791044777e-05,\n",
       "  'epoch': 0.97,\n",
       "  'step': 177},\n",
       " {'loss': 0.6687,\n",
       "  'learning_rate': 6.865671641791044e-05,\n",
       "  'epoch': 0.99,\n",
       "  'step': 180},\n",
       " {'loss': 0.7786,\n",
       "  'learning_rate': 6.809701492537313e-05,\n",
       "  'epoch': 1.01,\n",
       "  'step': 183},\n",
       " {'loss': 0.5915,\n",
       "  'learning_rate': 6.753731343283583e-05,\n",
       "  'epoch': 1.02,\n",
       "  'step': 186},\n",
       " {'loss': 0.6749,\n",
       "  'learning_rate': 6.697761194029852e-05,\n",
       "  'epoch': 1.04,\n",
       "  'step': 189},\n",
       " {'loss': 0.6933,\n",
       "  'learning_rate': 6.64179104477612e-05,\n",
       "  'epoch': 1.05,\n",
       "  'step': 192},\n",
       " {'loss': 0.708,\n",
       "  'learning_rate': 6.585820895522388e-05,\n",
       "  'epoch': 1.07,\n",
       "  'step': 195},\n",
       " {'loss': 0.7584,\n",
       "  'learning_rate': 6.529850746268657e-05,\n",
       "  'epoch': 1.09,\n",
       "  'step': 198},\n",
       " {'loss': 0.542,\n",
       "  'learning_rate': 6.473880597014925e-05,\n",
       "  'epoch': 1.1,\n",
       "  'step': 201},\n",
       " {'loss': 0.7593,\n",
       "  'learning_rate': 6.417910447761194e-05,\n",
       "  'epoch': 1.12,\n",
       "  'step': 204},\n",
       " {'loss': 0.6641,\n",
       "  'learning_rate': 6.361940298507463e-05,\n",
       "  'epoch': 1.14,\n",
       "  'step': 207},\n",
       " {'loss': 0.6208,\n",
       "  'learning_rate': 6.305970149253731e-05,\n",
       "  'epoch': 1.15,\n",
       "  'step': 210},\n",
       " {'loss': 0.6349, 'learning_rate': 6.25e-05, 'epoch': 1.17, 'step': 213},\n",
       " {'loss': 0.7893,\n",
       "  'learning_rate': 6.194029850746269e-05,\n",
       "  'epoch': 1.19,\n",
       "  'step': 216},\n",
       " {'loss': 0.714,\n",
       "  'learning_rate': 6.138059701492538e-05,\n",
       "  'epoch': 1.2,\n",
       "  'step': 219},\n",
       " {'loss': 0.6764,\n",
       "  'learning_rate': 6.082089552238807e-05,\n",
       "  'epoch': 1.22,\n",
       "  'step': 222},\n",
       " {'loss': 0.3943,\n",
       "  'learning_rate': 6.026119402985075e-05,\n",
       "  'epoch': 1.24,\n",
       "  'step': 225},\n",
       " {'loss': 0.7233,\n",
       "  'learning_rate': 5.970149253731343e-05,\n",
       "  'epoch': 1.25,\n",
       "  'step': 228},\n",
       " {'loss': 0.5597,\n",
       "  'learning_rate': 5.914179104477612e-05,\n",
       "  'epoch': 1.27,\n",
       "  'step': 231},\n",
       " {'loss': 0.5887,\n",
       "  'learning_rate': 5.85820895522388e-05,\n",
       "  'epoch': 1.29,\n",
       "  'step': 234},\n",
       " {'loss': 0.7746,\n",
       "  'learning_rate': 5.8022388059701494e-05,\n",
       "  'epoch': 1.3,\n",
       "  'step': 237},\n",
       " {'loss': 0.6367,\n",
       "  'learning_rate': 5.7462686567164184e-05,\n",
       "  'epoch': 1.32,\n",
       "  'step': 240},\n",
       " {'loss': 0.5034,\n",
       "  'learning_rate': 5.690298507462687e-05,\n",
       "  'epoch': 1.34,\n",
       "  'step': 243},\n",
       " {'loss': 0.4961,\n",
       "  'learning_rate': 5.634328358208956e-05,\n",
       "  'epoch': 1.35,\n",
       "  'step': 246},\n",
       " {'loss': 0.6148,\n",
       "  'learning_rate': 5.578358208955224e-05,\n",
       "  'epoch': 1.37,\n",
       "  'step': 249},\n",
       " {'loss': 0.4323,\n",
       "  'learning_rate': 5.5223880597014934e-05,\n",
       "  'epoch': 1.38,\n",
       "  'step': 252},\n",
       " {'loss': 0.516,\n",
       "  'learning_rate': 5.466417910447762e-05,\n",
       "  'epoch': 1.4,\n",
       "  'step': 255},\n",
       " {'loss': 0.7348,\n",
       "  'learning_rate': 5.4104477611940295e-05,\n",
       "  'epoch': 1.42,\n",
       "  'step': 258},\n",
       " {'loss': 0.8141,\n",
       "  'learning_rate': 5.3544776119402986e-05,\n",
       "  'epoch': 1.43,\n",
       "  'step': 261},\n",
       " {'loss': 0.61,\n",
       "  'learning_rate': 5.298507462686567e-05,\n",
       "  'epoch': 1.45,\n",
       "  'step': 264},\n",
       " {'loss': 0.6501,\n",
       "  'learning_rate': 5.242537313432836e-05,\n",
       "  'epoch': 1.47,\n",
       "  'step': 267},\n",
       " {'loss': 0.6805,\n",
       "  'learning_rate': 5.1865671641791044e-05,\n",
       "  'epoch': 1.48,\n",
       "  'step': 270},\n",
       " {'loss': 0.6262,\n",
       "  'learning_rate': 5.1305970149253735e-05,\n",
       "  'epoch': 1.5,\n",
       "  'step': 273},\n",
       " {'loss': 0.5524,\n",
       "  'learning_rate': 5.074626865671642e-05,\n",
       "  'epoch': 1.52,\n",
       "  'step': 276},\n",
       " {'loss': 0.7335,\n",
       "  'learning_rate': 5.018656716417911e-05,\n",
       "  'epoch': 1.53,\n",
       "  'step': 279},\n",
       " {'loss': 0.6384,\n",
       "  'learning_rate': 4.9626865671641794e-05,\n",
       "  'epoch': 1.55,\n",
       "  'step': 282},\n",
       " {'loss': 0.5496,\n",
       "  'learning_rate': 4.906716417910448e-05,\n",
       "  'epoch': 1.57,\n",
       "  'step': 285},\n",
       " {'loss': 0.4332,\n",
       "  'learning_rate': 4.850746268656717e-05,\n",
       "  'epoch': 1.58,\n",
       "  'step': 288},\n",
       " {'loss': 0.595,\n",
       "  'learning_rate': 4.794776119402985e-05,\n",
       "  'epoch': 1.6,\n",
       "  'step': 291},\n",
       " {'loss': 0.6549,\n",
       "  'learning_rate': 4.738805970149254e-05,\n",
       "  'epoch': 1.62,\n",
       "  'step': 294},\n",
       " {'loss': 0.8254,\n",
       "  'learning_rate': 4.682835820895523e-05,\n",
       "  'epoch': 1.63,\n",
       "  'step': 297},\n",
       " {'loss': 0.4249,\n",
       "  'learning_rate': 4.626865671641791e-05,\n",
       "  'epoch': 1.65,\n",
       "  'step': 300},\n",
       " {'loss': 0.8119,\n",
       "  'learning_rate': 4.57089552238806e-05,\n",
       "  'epoch': 1.66,\n",
       "  'step': 303},\n",
       " {'loss': 0.6878,\n",
       "  'learning_rate': 4.5149253731343286e-05,\n",
       "  'epoch': 1.68,\n",
       "  'step': 306},\n",
       " {'loss': 0.5179,\n",
       "  'learning_rate': 4.458955223880597e-05,\n",
       "  'epoch': 1.7,\n",
       "  'step': 309},\n",
       " {'loss': 0.6308,\n",
       "  'learning_rate': 4.402985074626866e-05,\n",
       "  'epoch': 1.71,\n",
       "  'step': 312},\n",
       " {'loss': 0.5825,\n",
       "  'learning_rate': 4.3470149253731345e-05,\n",
       "  'epoch': 1.73,\n",
       "  'step': 315},\n",
       " {'loss': 0.5277,\n",
       "  'learning_rate': 4.2910447761194036e-05,\n",
       "  'epoch': 1.75,\n",
       "  'step': 318},\n",
       " {'loss': 0.5026,\n",
       "  'learning_rate': 4.235074626865671e-05,\n",
       "  'epoch': 1.76,\n",
       "  'step': 321},\n",
       " {'loss': 0.5554,\n",
       "  'learning_rate': 4.1791044776119404e-05,\n",
       "  'epoch': 1.78,\n",
       "  'step': 324},\n",
       " {'loss': 0.7063,\n",
       "  'learning_rate': 4.1231343283582094e-05,\n",
       "  'epoch': 1.8,\n",
       "  'step': 327},\n",
       " {'loss': 0.6788,\n",
       "  'learning_rate': 4.067164179104478e-05,\n",
       "  'epoch': 1.81,\n",
       "  'step': 330},\n",
       " {'loss': 0.5645,\n",
       "  'learning_rate': 4.011194029850747e-05,\n",
       "  'epoch': 1.83,\n",
       "  'step': 333},\n",
       " {'loss': 0.4647,\n",
       "  'learning_rate': 3.9552238805970146e-05,\n",
       "  'epoch': 1.85,\n",
       "  'step': 336},\n",
       " {'loss': 0.6296,\n",
       "  'learning_rate': 3.899253731343284e-05,\n",
       "  'epoch': 1.86,\n",
       "  'step': 339},\n",
       " {'loss': 0.4405,\n",
       "  'learning_rate': 3.843283582089552e-05,\n",
       "  'epoch': 1.88,\n",
       "  'step': 342},\n",
       " {'loss': 0.4422,\n",
       "  'learning_rate': 3.787313432835821e-05,\n",
       "  'epoch': 1.9,\n",
       "  'step': 345},\n",
       " {'loss': 0.4153,\n",
       "  'learning_rate': 3.73134328358209e-05,\n",
       "  'epoch': 1.91,\n",
       "  'step': 348},\n",
       " {'loss': 0.4118,\n",
       "  'learning_rate': 3.675373134328358e-05,\n",
       "  'epoch': 1.93,\n",
       "  'step': 351},\n",
       " {'loss': 0.893,\n",
       "  'learning_rate': 3.619402985074627e-05,\n",
       "  'epoch': 1.95,\n",
       "  'step': 354},\n",
       " {'loss': 0.6946,\n",
       "  'learning_rate': 3.5634328358208955e-05,\n",
       "  'epoch': 1.96,\n",
       "  'step': 357},\n",
       " {'loss': 0.5714,\n",
       "  'learning_rate': 3.5074626865671645e-05,\n",
       "  'epoch': 1.98,\n",
       "  'step': 360},\n",
       " {'loss': 0.6316,\n",
       "  'learning_rate': 3.451492537313433e-05,\n",
       "  'epoch': 1.99,\n",
       "  'step': 363},\n",
       " {'loss': 0.5119,\n",
       "  'learning_rate': 3.395522388059701e-05,\n",
       "  'epoch': 2.01,\n",
       "  'step': 366},\n",
       " {'loss': 0.6211,\n",
       "  'learning_rate': 3.3395522388059704e-05,\n",
       "  'epoch': 2.03,\n",
       "  'step': 369},\n",
       " {'loss': 0.5671,\n",
       "  'learning_rate': 3.283582089552239e-05,\n",
       "  'epoch': 2.04,\n",
       "  'step': 372},\n",
       " {'loss': 0.524,\n",
       "  'learning_rate': 3.227611940298508e-05,\n",
       "  'epoch': 2.06,\n",
       "  'step': 375},\n",
       " {'loss': 0.4777,\n",
       "  'learning_rate': 3.171641791044776e-05,\n",
       "  'epoch': 2.08,\n",
       "  'step': 378},\n",
       " {'loss': 0.6933,\n",
       "  'learning_rate': 3.115671641791045e-05,\n",
       "  'epoch': 2.09,\n",
       "  'step': 381},\n",
       " {'loss': 0.5512,\n",
       "  'learning_rate': 3.059701492537314e-05,\n",
       "  'epoch': 2.11,\n",
       "  'step': 384},\n",
       " {'loss': 0.4859,\n",
       "  'learning_rate': 3.003731343283582e-05,\n",
       "  'epoch': 2.13,\n",
       "  'step': 387},\n",
       " {'loss': 0.5411,\n",
       "  'learning_rate': 2.9477611940298512e-05,\n",
       "  'epoch': 2.14,\n",
       "  'step': 390},\n",
       " {'loss': 0.5796,\n",
       "  'learning_rate': 2.8917910447761193e-05,\n",
       "  'epoch': 2.16,\n",
       "  'step': 393},\n",
       " {'loss': 0.4417,\n",
       "  'learning_rate': 2.835820895522388e-05,\n",
       "  'epoch': 2.18,\n",
       "  'step': 396},\n",
       " {'loss': 0.4631,\n",
       "  'learning_rate': 2.7798507462686568e-05,\n",
       "  'epoch': 2.19,\n",
       "  'step': 399},\n",
       " {'loss': 0.4614,\n",
       "  'learning_rate': 2.7238805970149255e-05,\n",
       "  'epoch': 2.21,\n",
       "  'step': 402},\n",
       " {'loss': 0.5138,\n",
       "  'learning_rate': 2.6679104477611942e-05,\n",
       "  'epoch': 2.23,\n",
       "  'step': 405},\n",
       " {'loss': 0.6696,\n",
       "  'learning_rate': 2.6119402985074626e-05,\n",
       "  'epoch': 2.24,\n",
       "  'step': 408},\n",
       " {'loss': 0.494,\n",
       "  'learning_rate': 2.5559701492537314e-05,\n",
       "  'epoch': 2.26,\n",
       "  'step': 411},\n",
       " {'loss': 0.56, 'learning_rate': 2.5e-05, 'epoch': 2.27, 'step': 414},\n",
       " {'loss': 0.7987,\n",
       "  'learning_rate': 2.444029850746269e-05,\n",
       "  'epoch': 2.29,\n",
       "  'step': 417},\n",
       " {'loss': 0.731,\n",
       "  'learning_rate': 2.3880597014925373e-05,\n",
       "  'epoch': 2.31,\n",
       "  'step': 420},\n",
       " {'loss': 0.674,\n",
       "  'learning_rate': 2.332089552238806e-05,\n",
       "  'epoch': 2.32,\n",
       "  'step': 423},\n",
       " {'loss': 0.647,\n",
       "  'learning_rate': 2.2761194029850747e-05,\n",
       "  'epoch': 2.34,\n",
       "  'step': 426},\n",
       " {'loss': 0.7021,\n",
       "  'learning_rate': 2.2201492537313435e-05,\n",
       "  'epoch': 2.36,\n",
       "  'step': 429},\n",
       " {'loss': 0.7945,\n",
       "  'learning_rate': 2.164179104477612e-05,\n",
       "  'epoch': 2.37,\n",
       "  'step': 432},\n",
       " {'loss': 0.3833,\n",
       "  'learning_rate': 2.1082089552238806e-05,\n",
       "  'epoch': 2.39,\n",
       "  'step': 435},\n",
       " {'loss': 0.8692,\n",
       "  'learning_rate': 2.0522388059701493e-05,\n",
       "  'epoch': 2.41,\n",
       "  'step': 438},\n",
       " {'loss': 0.3992,\n",
       "  'learning_rate': 1.996268656716418e-05,\n",
       "  'epoch': 2.42,\n",
       "  'step': 441},\n",
       " {'loss': 0.6394,\n",
       "  'learning_rate': 1.9402985074626868e-05,\n",
       "  'epoch': 2.44,\n",
       "  'step': 444},\n",
       " {'loss': 0.5334,\n",
       "  'learning_rate': 1.8843283582089552e-05,\n",
       "  'epoch': 2.46,\n",
       "  'step': 447},\n",
       " {'loss': 0.5466,\n",
       "  'learning_rate': 1.828358208955224e-05,\n",
       "  'epoch': 2.47,\n",
       "  'step': 450},\n",
       " {'loss': 0.3829,\n",
       "  'learning_rate': 1.7723880597014927e-05,\n",
       "  'epoch': 2.49,\n",
       "  'step': 453},\n",
       " {'loss': 0.7103,\n",
       "  'learning_rate': 1.716417910447761e-05,\n",
       "  'epoch': 2.51,\n",
       "  'step': 456},\n",
       " {'loss': 0.6934,\n",
       "  'learning_rate': 1.66044776119403e-05,\n",
       "  'epoch': 2.52,\n",
       "  'step': 459},\n",
       " {'loss': 0.4685,\n",
       "  'learning_rate': 1.6044776119402986e-05,\n",
       "  'epoch': 2.54,\n",
       "  'step': 462},\n",
       " {'loss': 0.6265,\n",
       "  'learning_rate': 1.5485074626865673e-05,\n",
       "  'epoch': 2.55,\n",
       "  'step': 465},\n",
       " {'loss': 0.6323,\n",
       "  'learning_rate': 1.4925373134328357e-05,\n",
       "  'epoch': 2.57,\n",
       "  'step': 468},\n",
       " {'loss': 0.6216,\n",
       "  'learning_rate': 1.4365671641791046e-05,\n",
       "  'epoch': 2.59,\n",
       "  'step': 471},\n",
       " {'loss': 0.579,\n",
       "  'learning_rate': 1.3805970149253733e-05,\n",
       "  'epoch': 2.6,\n",
       "  'step': 474},\n",
       " {'loss': 0.5266,\n",
       "  'learning_rate': 1.3246268656716417e-05,\n",
       "  'epoch': 2.62,\n",
       "  'step': 477},\n",
       " {'loss': 0.5324,\n",
       "  'learning_rate': 1.2686567164179105e-05,\n",
       "  'epoch': 2.64,\n",
       "  'step': 480},\n",
       " {'loss': 0.6893,\n",
       "  'learning_rate': 1.2126865671641792e-05,\n",
       "  'epoch': 2.65,\n",
       "  'step': 483},\n",
       " {'loss': 0.6302,\n",
       "  'learning_rate': 1.1567164179104478e-05,\n",
       "  'epoch': 2.67,\n",
       "  'step': 486},\n",
       " {'loss': 0.5513,\n",
       "  'learning_rate': 1.1007462686567165e-05,\n",
       "  'epoch': 2.69,\n",
       "  'step': 489},\n",
       " {'loss': 0.5587,\n",
       "  'learning_rate': 1.0447761194029851e-05,\n",
       "  'epoch': 2.7,\n",
       "  'step': 492},\n",
       " {'loss': 0.4864,\n",
       "  'learning_rate': 9.888059701492537e-06,\n",
       "  'epoch': 2.72,\n",
       "  'step': 495},\n",
       " {'loss': 0.6862,\n",
       "  'learning_rate': 9.328358208955226e-06,\n",
       "  'epoch': 2.74,\n",
       "  'step': 498},\n",
       " {'loss': 0.5371,\n",
       "  'learning_rate': 8.768656716417911e-06,\n",
       "  'epoch': 2.75,\n",
       "  'step': 501},\n",
       " {'loss': 0.521,\n",
       "  'learning_rate': 8.208955223880597e-06,\n",
       "  'epoch': 2.77,\n",
       "  'step': 504},\n",
       " {'loss': 0.645,\n",
       "  'learning_rate': 7.649253731343284e-06,\n",
       "  'epoch': 2.79,\n",
       "  'step': 507},\n",
       " {'loss': 0.6043,\n",
       "  'learning_rate': 7.08955223880597e-06,\n",
       "  'epoch': 2.8,\n",
       "  'step': 510},\n",
       " {'loss': 0.484,\n",
       "  'learning_rate': 6.529850746268657e-06,\n",
       "  'epoch': 2.82,\n",
       "  'step': 513},\n",
       " {'loss': 0.4056,\n",
       "  'learning_rate': 5.970149253731343e-06,\n",
       "  'epoch': 2.84,\n",
       "  'step': 516},\n",
       " {'loss': 0.6385,\n",
       "  'learning_rate': 5.41044776119403e-06,\n",
       "  'epoch': 2.85,\n",
       "  'step': 519},\n",
       " {'loss': 0.6882,\n",
       "  'learning_rate': 4.850746268656717e-06,\n",
       "  'epoch': 2.87,\n",
       "  'step': 522},\n",
       " {'loss': 0.6285,\n",
       "  'learning_rate': 4.291044776119403e-06,\n",
       "  'epoch': 2.88,\n",
       "  'step': 525},\n",
       " {'loss': 0.5652,\n",
       "  'learning_rate': 3.7313432835820893e-06,\n",
       "  'epoch': 2.9,\n",
       "  'step': 528},\n",
       " {'loss': 0.6978,\n",
       "  'learning_rate': 3.171641791044776e-06,\n",
       "  'epoch': 2.92,\n",
       "  'step': 531},\n",
       " {'loss': 0.4539,\n",
       "  'learning_rate': 2.6119402985074627e-06,\n",
       "  'epoch': 2.93,\n",
       "  'step': 534},\n",
       " {'loss': 0.6191,\n",
       "  'learning_rate': 2.0522388059701493e-06,\n",
       "  'epoch': 2.95,\n",
       "  'step': 537},\n",
       " {'loss': 0.4919,\n",
       "  'learning_rate': 1.4925373134328358e-06,\n",
       "  'epoch': 2.97,\n",
       "  'step': 540},\n",
       " {'loss': 0.4385,\n",
       "  'learning_rate': 9.328358208955223e-07,\n",
       "  'epoch': 2.98,\n",
       "  'step': 543},\n",
       " {'loss': 0.4104,\n",
       "  'learning_rate': 3.7313432835820895e-07,\n",
       "  'epoch': 3.0,\n",
       "  'step': 546}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: \"loss\" in x, trainer.state.log_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the training loss and steps from the log history\n",
    "train_loss = [log[\"loss\"] for log in list(filter(lambda x: \"loss\" in x, trainer.state.log_history))]\n",
    "steps = [log[\"step\"] for log in list(filter(lambda x: \"loss\" in x, trainer.state.log_history))]\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(steps, train_loss)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Steps\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModel' is not supported for . Supported models are ['Pop2PianoForConditionalGeneration', 'SeamlessM4TForSpeechToText', 'SeamlessM4Tv2ForSpeechToText', 'SpeechEncoderDecoderModel', 'Speech2TextForConditionalGeneration', 'SpeechT5ForSpeechToText', 'WhisperForConditionalGeneration', 'Data2VecAudioForCTC', 'HubertForCTC', 'MCTCTForCTC', 'SEWForCTC', 'SEWDForCTC', 'UniSpeechForCTC', 'UniSpeechSatForCTC', 'Wav2Vec2ForCTC', 'Wav2Vec2ConformerForCTC', 'WavLMForCTC'].\n"
     ]
    }
   ],
   "source": [
    "test_audio = \"../../peft/data/audio/test_zh.flac\"\n",
    "\n",
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "\n",
    "pipeline = AutomaticSpeechRecognitionPipeline(model=model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"chinese\", task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/python-env/learn-llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/sheng/python-env/learn-llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/sheng/python-env/learn-llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一段测试用于WhisperLarge V2模型的自动语音识别测试。\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    text = pipeline(test_audio, generate_kwargs={\"forced_decoder_ids\": forced_decoder_ids}, max_new_tokens=255)[\"text\"]\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 在“LoRA 低秩适配 OpenAI Whisper-Large-V2 语音识别任务”中，当 LoRA 模型训练完成后，使用测试集进行完整的模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): WhisperForConditionalGeneration(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51865, 1280, padding_idx=50257)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1280, out_features=51865, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# 词错误率（WER）是评估ASR模型常用的指标。从 Evaluate加载 WER 指标\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [56:05<00:00, 12.70s/it] \n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = (\n",
    "                model.generate(\n",
    "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                    decoder_input_ids=batch[\"labels\"][:, :4].to(\"cuda\"),\n",
    "                    max_new_tokens=255,\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            metric.add_batch(\n",
    "                predictions=decoded_preds,\n",
    "                references=decoded_labels,\n",
    "            )\n",
    "    del generated_tokens, labels, batch\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wer=62.19281663516069\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wer = 100 * metric.compute()\n",
    "print(f\"{wer=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
